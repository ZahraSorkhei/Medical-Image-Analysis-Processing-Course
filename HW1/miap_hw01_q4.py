# -*- coding: utf-8 -*-
"""miap_hw01_q4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nUg0Yr2hUYo7_Bp0TZrBdnX2pk1blVir

### Q4
"""

#Import Libraris
import os
import sys 
import cv2 as cv
import numpy as np
import plotly.io as pio
import plotly.graph_objs as go
from PIL import Image
from skimage import color
from plotly import subplots
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

#Loading data 
IMG_DIR = '/content/drive/MyDrive/brain_data'
X = []
X_flat = []

size = 96


for img in os.listdir(IMG_DIR):

    img_array = cv.imread(os.path.join(IMG_DIR, img), cv.IMREAD_GRAYSCALE)
    img_64 = np.array(Image.fromarray(img_array).resize((size, size), Image.ANTIALIAS))
    img_64 = img_64.flatten()
    X_flat.append(img_64)
X_flat = np.asarray(X_flat)
X_flat = X_flat.T
X_flat = X_flat.astype('int')

#make sure that the shape of data is correct
X_flat.shape

#ploting original images
fig = plt.figure(figsize=(20, 10))
rows = 3
columns = 5
for i in range(15):
  fig.add_subplot(rows, columns, i+1)
  plt.imshow(X_flat[:,i].reshape((96,96)).astype("uint8"),cmap = 'gray')
  plt.axis('off')
  plt.title("image "+str(i+1))
plt.show()

X_flat = X_flat.astype('float')

"""#### method 1"""

# Define a function for PCA and reconstruction
def my_pca_recon(X):
  # returns transformed X, reconstructed X, variance explained
  mean = np.mean(X, axis=0) # compute column mean 
  Z = X - mean # center the data 
  cov = np.dot(Z.T, Z) / (X.shape[0] - 1) # compute covariance matrix 
  evals, evecs = np.linalg.eig(cov) # compute eigenvalues and eigenvectors 
  idx = np.argsort(evals)[::-1] # sort eigenvalues in descending order 
  evecs = evecs[:,idx] # sort eigenvectors accordingly 
  evals = evals[idx] # sort eigenvalues accordingly 
  # We use cumsum to find the best number of components
  variance_retained=np.cumsum(evals)/np.sum(evals)
  n_components = np.argmax(variance_retained>=0.99)+1

  X_pca = np.dot(Z, evecs[:,0:n_components]) # project data onto first n_components eigenvectors 

  X_recon = np.dot(X_pca.reshape(-1,n_components), evecs[:,0:n_components].T) + mean # inverse transform using first n_components eigenvectors 

  


  return X_recon,n_components

X_recon,n_components  = my_pca_recon (X_flat)

print("Number of eigen vectors is = ",n_components )

#ploting recunstructed images
fig = plt.figure(figsize=(20, 10))
rows = 3
columns = 5
for i in range(15):
  fig.add_subplot(rows, columns, i+1)
  plt.imshow(X_recon[:,i].reshape((96,96)).astype("uint8"),cmap = 'gray')
  plt.axis('off')
  plt.title("image "+str(i+1))
plt.show()

"""#### method 2"""

#Standardizing data
mean = np.mean(X_flat)
std = np.std(X_flat)
standardized_X = ((X_flat - mean) / std)

standardized_X.shape

#Find covariance
C = np.cov(standardized_X)
#Find eigen values and eighen vectors
eig_val, eig_vec = np.linalg.eigh(C) 
#ŸèSort based on eigen values 
sorted_eig  = np.argsort(-eig_val)
eig_val = eig_val[sorted_eig]
eig_vec = eig_vec[:, sorted_eig]

"""Now we want to find the best number of components"""

# Cumulative sum of eigenvalues; This will be used to create step plot
# for visualizing the variance explained by each principal component.
cum_sum_eig_val = np.cumsum(eig_val)/np.sum(eig_val)

"""We plot cumsum of eigen values and peak eigen vectors until this sum become 1"""

plt.plot(list(range(1, len(cum_sum_eig_val))[:20]), cum_sum_eig_val[:20]) 
plt.show()

"""So we choose 14 as number of components"""

#calculating loss and reconstructing images
loss = []
reconstructions = []
max_components = 20

print("Processing...")
for num_component in range(1, max_components + 1):
    print("num_component = ",num_component," of ",max_components)
    A = eig_vec[:, range(num_component)]
    P = A @ A.T 
    #Find reconsruction
    reconst = standardized_X.T @ P
    #Find Error of reconsruction
    error = np.sqrt(np.square(reconst - standardized_X.T).sum(axis = 1)).mean()
    reconstructions.append(reconst)
    loss.append((num_component, error))
print()
print("Done!")

#visualizing mse vs number of principal components
loss = np.array(loss)
trace = go.Scatter(x = loss[:, 0], y = loss[:, 1])
data = [trace]
fig = go.Figure(data)
fig.update_layout(title = "MSE vs number of principal components", 
                  xaxis_title = "Number of principal components", 
                  yaxis_title = "MSE", template = "plotly_dark")
fig.show()

"""We peak 12 as number of eigen vectors"""

A = eig_vec[:, range(14)]
P = A @ A.T 
reconst = standardized_X.T @ P
reconst2 = reconst*std + mean

fig = plt.figure(figsize=(20, 10))
rows = 3
columns = 5
for i in range(15):
  fig.add_subplot(rows, columns, i+1)
  plt.imshow(reconst2[i,:].reshape((96,96)).astype("uint8"),cmap = 'gray')
  plt.axis('off')
  plt.title("image "+str(i+1))
plt.show()

"""#### Advantages & Disadvantages

***Some of the advantages are:***

*   It can counteract the issues of a high-dimensional data set, such as 
overfitting and computational cost.


*   It can remove correlated features and reduce noise in the data.


*   It can speed up other machine learning algorithms by using fewer features.


*   It can improve visualization by transforming high-dimensional data into low-dimensional data.


***Some of the disadvantages are:***

*   It requires data normalization before performing PCA, otherwise variables with larger scales would dominate the PCA.


*   It may lose some valuable information if not enough principal components are selected to capture the variance in the data.


*   It may produce difficult to interpret principal components that are 
linear combinations of original features.

*   Low interpretability of principal components. Principal components are linear combinations of the features from the original data, but they are not as easy to interpret. For example, it is difficult to tell which are the most important features in the dataset after computing principal components.
"""